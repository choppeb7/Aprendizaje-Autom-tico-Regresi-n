{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400a64f7",
   "metadata": {},
   "source": [
    "# Regresión Lineal Múltiple: Análisis de Residuos y Tratamiento de Outliers\n",
    "\n",
    "En este ejercicio trabajaremos con el dataset de Boston Housing para:\n",
    "\n",
    "1. **Análisis de residuos**: Evaluación de supuestos de la regresión lineal\n",
    "2. **Detección de heterocedasticidad**: Pruebas estadísticas y visualizaciones\n",
    "3. **Evaluación de normalidad**: Pruebas de normalidad en los residuos\n",
    "4. **Detección de outliers**: Métodos estadísticos y visuales\n",
    "5. **Transformación de variables**: Normalización, estandarización, escalado Min-Max\n",
    "6. **Evaluación del impacto**: Comparación de la calidad de residuos antes y después del tratamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46292169",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979174e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera, shapiro, anderson, normaltest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de gráficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada184a",
   "metadata": {},
   "source": [
    "## Cargar y preparar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd60aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "dataset = pd.read_csv('../data/BostonHousing.csv')\n",
    "\n",
    "# Separar variables predictoras y objetivo\n",
    "X = dataset.drop('MEDV', axis=1)\n",
    "y = dataset['MEDV']\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8298a3",
   "metadata": {},
   "source": [
    "## 1. Modelo Base y Análisis de Residuos Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo base\n",
    "model_base = LinearRegression()\n",
    "model_base.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_train = model_base.predict(X_train)\n",
    "y_pred_test = model_base.predict(X_test)\n",
    "\n",
    "# Calcular residuos\n",
    "residuals_train = y_train - y_pred_train\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "# Métricas del modelo base\n",
    "mse_base = mean_squared_error(y_test, y_pred_test)\n",
    "r2_base = r2_score(y_test, y_pred_test)\n",
    "rmse_base = np.sqrt(mse_base)\n",
    "\n",
    "print(\"=== MODELO BASE ===\")\n",
    "print(f\"MSE: {mse_base:.3f}\")\n",
    "print(f\"RMSE: {rmse_base:.3f}\")\n",
    "print(f\"R²: {r2_base:.3f}\")\n",
    "print(f\"\\nEstadísticas de los residuos:\")\n",
    "print(f\"Media: {np.mean(residuals_test):.6f}\")\n",
    "print(f\"Desviación estándar: {np.std(residuals_test):.3f}\")\n",
    "print(f\"Mínimo: {np.min(residuals_test):.3f}\")\n",
    "print(f\"Máximo: {np.max(residuals_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745933d2",
   "metadata": {},
   "source": [
    "### 1.1 Visualización de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe405249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gráficos de diagnóstico de residuos\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Residuos vs Valores Predichos\n",
    "ax1.scatter(y_pred_test, residuals_test, alpha=0.6)\n",
    "ax1.axhline(y=0, color='red', linestyle='--')\n",
    "ax1.set_xlabel('Valores Predichos')\n",
    "ax1.set_ylabel('Residuos')\n",
    "ax1.set_title('Residuos vs Valores Predichos')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Valores Reales vs Predichos\n",
    "ax2.scatter(y_test, y_pred_test, alpha=0.6)\n",
    "min_val = min(y_test.min(), y_pred_test.min())\n",
    "max_val = max(y_test.max(), y_pred_test.max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--')\n",
    "ax2.set_xlabel('Valores Reales')\n",
    "ax2.set_ylabel('Valores Predichos')\n",
    "ax2.set_title('Valores Reales vs Predichos')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot de residuos\n",
    "stats.probplot(residuals_test, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot de Residuos')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Histograma de residuos\n",
    "ax4.hist(residuals_test, bins=20, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "# Superimponer curva normal\n",
    "mu, sigma = stats.norm.fit(residuals_test)\n",
    "x = np.linspace(residuals_test.min(), residuals_test.max(), 100)\n",
    "ax4.plot(x, stats.norm.pdf(x, mu, sigma), 'red', linewidth=2, label='Normal')\n",
    "ax4.set_xlabel('Residuos')\n",
    "ax4.set_ylabel('Densidad')\n",
    "ax4.set_title('Distribución de Residuos')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Análisis de Residuos - Modelo Base', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f305fff",
   "metadata": {},
   "source": [
    "## 2. Pruebas de Heterocedasticidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bfe922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para pruebas estadísticas\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "model_sm = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "print(\"=== PRUEBAS DE HETEROCEDASTICIDAD ===\")\n",
    "\n",
    "# 1. Prueba de Breusch-Pagan\n",
    "bp_stat, bp_pvalue, bp_fstat, bp_f_pvalue = het_breuschpagan(model_sm.resid, X_train_const)\n",
    "print(f\"\\n1. Prueba de Breusch-Pagan:\")\n",
    "print(f\"   Estadístico LM: {bp_stat:.4f}\")\n",
    "print(f\"   p-valor: {bp_pvalue:.4f}\")\n",
    "print(f\"   Interpretación: {'Hay heterocedasticidad' if bp_pvalue < 0.05 else 'Homocedasticidad (varianza constante)'}\")\n",
    "\n",
    "# 2. Prueba de White\n",
    "white_stat, white_pvalue, white_fstat, white_f_pvalue = het_white(model_sm.resid, X_train_const)\n",
    "print(f\"\\n2. Prueba de White:\")\n",
    "print(f\"   Estadístico LM: {white_stat:.4f}\")\n",
    "print(f\"   p-valor: {white_pvalue:.4f}\")\n",
    "print(f\"   Interpretación: {'Hay heterocedasticidad' if white_pvalue < 0.05 else 'Homocedasticidad (varianza constante)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9615971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis visual de heterocedasticidad\n",
    "# Calcular el número de subplots necesarios\n",
    "n_features = X_train.shape[1]\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "\n",
    "for i, feature in enumerate(X_train.columns):\n",
    "    # Crear scatter plot de residuos vs cada variable\n",
    "    axes[i].scatter(X_train[feature], residuals_train, alpha=0.6)\n",
    "    \n",
    "    axes[i].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Residuos')\n",
    "    axes[i].set_title(f'Residuos vs {feature}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Ocultar subplots sobrantes\n",
    "for i in range(n_features, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.suptitle('Análisis Visual de Heterocedasticidad', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63712c02",
   "metadata": {},
   "source": [
    "## 3. Pruebas de Normalidad de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PRUEBAS DE NORMALIDAD DE RESIDUOS ===\")\n",
    "\n",
    "# 1. Prueba de Shapiro-Wilk\n",
    "shapiro_stat, shapiro_pvalue = shapiro(residuals_test)\n",
    "print(f\"\\n1. Prueba de Shapiro-Wilk:\")\n",
    "print(f\"   Estadístico W: {shapiro_stat:.4f}\")\n",
    "print(f\"   p-valor: {shapiro_pvalue:.4f}\")\n",
    "print(f\"   Interpretación: {'Los residuos NO son normales' if shapiro_pvalue < 0.05 else 'Los residuos siguen distribución normal'}\")\n",
    "\n",
    "# 2. Prueba de Jarque-Bera\n",
    "jb_stat, jb_pvalue = jarque_bera(residuals_test)\n",
    "print(f\"\\n2. Prueba de Jarque-Bera:\")\n",
    "print(f\"   Estadístico JB: {jb_stat:.4f}\")\n",
    "print(f\"   p-valor: {jb_pvalue:.4f}\")\n",
    "print(f\"   Interpretación: {'Los residuos NO son normales' if jb_pvalue < 0.05 else 'Los residuos siguen distribución normal'}\")\n",
    "\n",
    "# 3. Prueba de D'Agostino\n",
    "dagostino_stat, dagostino_pvalue = normaltest(residuals_test)\n",
    "print(f\"\\n3. Prueba de D'Agostino:\")\n",
    "print(f\"   Estadístico: {dagostino_stat:.4f}\")\n",
    "print(f\"   p-valor: {dagostino_pvalue:.4f}\")\n",
    "print(f\"   Interpretación: {'Los residuos NO son normales' if dagostino_pvalue < 0.05 else 'Los residuos siguen distribución normal'}\")\n",
    "\n",
    "# 4. Prueba de Anderson-Darling\n",
    "anderson_result = anderson(residuals_test, dist='norm')\n",
    "print(f\"\\n4. Prueba de Anderson-Darling:\")\n",
    "print(f\"   Estadístico AD: {anderson_result.statistic:.4f}\")\n",
    "print(f\"   Valores críticos: {anderson_result.critical_values}\")\n",
    "print(f\"   Niveles de significancia: {anderson_result.significance_level}\")\n",
    "\n",
    "# Interpretación de Anderson-Darling\n",
    "reject_levels = []\n",
    "for i, cv in enumerate(anderson_result.critical_values):\n",
    "    if anderson_result.statistic > cv:\n",
    "        reject_levels.append(anderson_result.significance_level[i])\n",
    "\n",
    "if reject_levels:\n",
    "    print(f\"   Interpretación: Se rechaza normalidad en niveles: {reject_levels}%\")\n",
    "else:\n",
    "    print(f\"   Interpretación: No se rechaza normalidad en ningún nivel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e91b5",
   "metadata": {},
   "source": [
    "## 4. Detección de Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a22cd",
   "metadata": {},
   "source": [
    "### 4.1 Métodos Univariados (Variable Objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa597a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando IQR en la variable objetivo\n",
    "Q1 = y.quantile(0.25)\n",
    "Q3 = y.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Límites para outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identificar outliers\n",
    "outliers_iqr = (y < lower_bound) | (y > upper_bound)\n",
    "\n",
    "print(\"=== DETECCIÓN DE OUTLIERS (IQR) ===\")\n",
    "print(f\"Q1: {Q1:.2f}\")\n",
    "print(f\"Q3: {Q3:.2f}\")\n",
    "print(f\"IQR: {IQR:.2f}\")\n",
    "print(f\"Límite inferior: {lower_bound:.2f}\")\n",
    "print(f\"Límite superior: {upper_bound:.2f}\")\n",
    "print(f\"Número de outliers detectados: {np.sum(outliers_iqr)}\")\n",
    "print(f\"Porcentaje de outliers: {100 * np.sum(outliers_iqr) / len(y):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c869853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de outliers usando Z-score\n",
    "z_scores = np.abs(stats.zscore(y))\n",
    "outliers_zscore = z_scores > 3\n",
    "\n",
    "print(\"=== DETECCIÓN DE OUTLIERS (Z-SCORE) ===\")\n",
    "print(f\"Número de outliers detectados (|z| > 3): {np.sum(outliers_zscore)}\")\n",
    "print(f\"Porcentaje de outliers: {100 * np.sum(outliers_zscore) / len(y):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de outliers\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Boxplot\n",
    "ax1.boxplot(y, vert=True)\n",
    "ax1.set_ylabel('MEDV (Precio de la vivienda)')\n",
    "ax1.set_title('Boxplot - Detección de Outliers')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Histograma con outliers marcados\n",
    "ax2.hist(y, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax2.axvline(lower_bound, color='red', linestyle='--', label=f'Límite inferior: {lower_bound:.2f}')\n",
    "ax2.axvline(upper_bound, color='red', linestyle='--', label=f'Límite superior: {upper_bound:.2f}')\n",
    "ax2.set_xlabel('MEDV (Precio de la vivienda)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_title('Distribución con Límites de Outliers (IQR)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot con outliers marcados\n",
    "normal_points = ~(outliers_iqr | outliers_zscore)\n",
    "ax3.scatter(range(len(y)), y[normal_points], alpha=0.6, label='Normal', color='blue')\n",
    "ax3.scatter(np.where(outliers_iqr)[0], y[outliers_iqr], alpha=0.8, label='Outliers IQR', color='red', s=50)\n",
    "ax3.scatter(np.where(outliers_zscore)[0], y[outliers_zscore], alpha=0.8, label='Outliers Z-score', color='orange', s=30, marker='^')\n",
    "ax3.set_xlabel('Índice de la observación')\n",
    "ax3.set_ylabel('MEDV (Precio de la vivienda)')\n",
    "ax3.set_title('Outliers Detectados por Diferentes Métodos')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d826ce4",
   "metadata": {},
   "source": [
    "### 4.2 Métodos Multivariados (Distancia de Mahalanobis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular distancia de Mahalanobis\n",
    "def mahalanobis_distance(X):\n",
    "    \"\"\"\n",
    "    Calcular la distancia de Mahalanobis para cada observación\n",
    "    \"\"\"\n",
    "    # Calcular media y matriz de covarianza\n",
    "    mean = np.mean(X, axis=0)\n",
    "    cov_matrix = np.cov(X, rowvar=False)\n",
    "    \n",
    "    # Calcular distancias de Mahalanobis\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(X.shape[0]):\n",
    "        # Calcular distancia para cada observación\n",
    "        diff = X.iloc[i] - mean\n",
    "        distance = np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    return np.array(distances)\n",
    "\n",
    "# Calcular distancias de Mahalanobis\n",
    "mahal_distances = mahalanobis_distance(X)\n",
    "\n",
    "# Usar percentil 95 como umbral para outliers\n",
    "threshold_mahal = np.percentile(mahal_distances, 95)\n",
    "outliers_mahal = mahal_distances > threshold_mahal\n",
    "\n",
    "print(\"=== DETECCIÓN DE OUTLIERS (MAHALANOBIS) ===\")\n",
    "print(f\"Umbral (percentil 95): {threshold_mahal:.3f}\")\n",
    "print(f\"Número de outliers detectados: {np.sum(outliers_mahal)}\")\n",
    "print(f\"Porcentaje de outliers: {100 * np.sum(outliers_mahal) / len(X):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc94b9",
   "metadata": {},
   "source": [
    "### 4.3 Análisis de Influencia (Leverage, Cook's Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de influencia usando statsmodels\n",
    "influence = OLSInfluence(model_sm)\n",
    "\n",
    "# Cook's Distance\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "# Leverage (Hat values)\n",
    "leverage = influence.hat_matrix_diag\n",
    "# Residuos estudentizados\n",
    "student_resid = influence.resid_studentized_external\n",
    "\n",
    "# Umbrales para detección de outliers\n",
    "cooks_threshold = 4 / len(X_train)  # Umbral común para Cook's D\n",
    "leverage_threshold = 2 * (X_train.shape[1] + 1) / len(X_train)  # Umbral para leverage\n",
    "student_threshold = 3  # Umbral para residuos estudentizados\n",
    "\n",
    "# Identificar outliers\n",
    "outliers_cooks = cooks_d > cooks_threshold\n",
    "outliers_leverage = leverage > leverage_threshold\n",
    "outliers_student = np.abs(student_resid) > student_threshold\n",
    "\n",
    "print(\"=== ANÁLISIS DE INFLUENCIA ===\")\n",
    "print(f\"\\nCook's Distance:\")\n",
    "print(f\"  Umbral: {cooks_threshold:.4f}\")\n",
    "print(f\"  Outliers detectados: {np.sum(outliers_cooks)}\")\n",
    "print(f\"  Porcentaje: {100 * np.sum(outliers_cooks) / len(X_train):.2f}%\")\n",
    "\n",
    "print(f\"\\nLeverage:\")\n",
    "print(f\"  Umbral: {leverage_threshold:.4f}\")\n",
    "print(f\"  Outliers detectados: {np.sum(outliers_leverage)}\")\n",
    "print(f\"  Porcentaje: {100 * np.sum(outliers_leverage) / len(X_train):.2f}%\")\n",
    "\n",
    "print(f\"\\nResiduos Estudentizados:\")\n",
    "print(f\"  Umbral: ±{student_threshold}\")\n",
    "print(f\"  Outliers detectados: {np.sum(outliers_student)}\")\n",
    "print(f\"  Porcentaje: {100 * np.sum(outliers_student) / len(X_train):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de métricas de influencia\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Cook's Distance\n",
    "ax1.bar(range(len(cooks_d)), cooks_d, alpha=0.7)\n",
    "ax1.axhline(y=cooks_threshold, color='red', linestyle='--', \n",
    "           label=f'Umbral: {cooks_threshold:.4f}')\n",
    "ax1.set_xlabel('Observación')\n",
    "ax1.set_ylabel(\"Cook's Distance\")\n",
    "ax1.set_title(\"Cook's Distance por Observación\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Leverage\n",
    "ax2.bar(range(len(leverage)), leverage, alpha=0.7, color='orange')\n",
    "ax2.axhline(y=leverage_threshold, color='red', linestyle='--',\n",
    "           label=f'Umbral: {leverage_threshold:.4f}')\n",
    "ax2.set_xlabel('Observación')\n",
    "ax2.set_ylabel('Leverage (Hat Values)')\n",
    "ax2.set_title('Leverage por Observación')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuos vs Leverage\n",
    "ax3.scatter(leverage, student_resid, alpha=0.6)\n",
    "ax3.axhline(y=student_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax3.axhline(y=-student_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax3.axvline(x=leverage_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax3.set_xlabel('Leverage')\n",
    "ax3.set_ylabel('Residuos Estudentizados')\n",
    "ax3.set_title('Residuos Estudentizados vs Leverage')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Cook's Distance vs Leverage\n",
    "ax4.scatter(leverage, cooks_d, alpha=0.6, color='green')\n",
    "ax4.axhline(y=cooks_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax4.axvline(x=leverage_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "ax4.set_xlabel('Leverage')\n",
    "ax4.set_ylabel(\"Cook's Distance\")\n",
    "ax4.set_title(\"Cook's Distance vs Leverage\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Análisis de Influencia y Outliers', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06f142",
   "metadata": {},
   "source": [
    "## 5. Transformaciones de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173cb25",
   "metadata": {},
   "source": [
    "### 5.1 Identificar Outliers Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un consenso de outliers basado en múltiples métodos\n",
    "# Combinar todos los métodos de detección de outliers\n",
    "outliers_combined = np.zeros(len(dataset), dtype=bool)\n",
    "\n",
    "# Marcar outliers del conjunto completo\n",
    "outliers_combined[outliers_iqr] = True\n",
    "outliers_combined[outliers_zscore] = True\n",
    "outliers_combined[outliers_mahal] = True\n",
    "\n",
    "# Para outliers de influencia, necesitamos mapear indices de entrenamiento\n",
    "train_indices = X_train.index\n",
    "outliers_combined[train_indices[outliers_cooks]] = True\n",
    "outliers_combined[train_indices[outliers_leverage]] = True\n",
    "outliers_combined[train_indices[outliers_student]] = True\n",
    "\n",
    "print(f\"Número total de outliers identificados: {np.sum(outliers_combined)}\")\n",
    "print(f\"Porcentaje del dataset: {100 * np.sum(outliers_combined) / len(dataset):.2f}%\")\n",
    "\n",
    "# Crear datasets con y sin outliers\n",
    "dataset_clean = dataset[~outliers_combined].copy()\n",
    "X_clean = dataset_clean.drop('MEDV', axis=1)\n",
    "y_clean = dataset_clean['MEDV']\n",
    "\n",
    "print(f\"\\nDimensiones originales: {dataset.shape}\")\n",
    "print(f\"Dimensiones después de limpieza: {dataset_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578225ba",
   "metadata": {},
   "source": [
    "### 5.2 Aplicar Diferentes Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788be538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diferentes transformadores\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'PowerTransformer_Yeo-Johnson': PowerTransformer(method='yeo-johnson'),\n",
    "    'PowerTransformer_Box-Cox': PowerTransformer(method='box-cox'),\n",
    "    'QuantileTransformer_Uniform': QuantileTransformer(output_distribution='uniform'),\n",
    "    'QuantileTransformer_Normal': QuantileTransformer(output_distribution='normal')\n",
    "}\n",
    "\n",
    "# Almacenar resultados de cada transformación\n",
    "transformation_results = []\n",
    "\n",
    "print(\"=== EVALUACIÓN DE TRANSFORMACIONES ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, scaler in scalers.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Aplicar transformación y evaluar modelo\n",
    "        # División de datos limpios\n",
    "        X_clean_train, X_clean_test, y_clean_train, y_clean_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Aplicar transformación\n",
    "        X_clean_train_scaled = scaler.fit_transform(X_clean_train)\n",
    "        X_clean_test_scaled = scaler.transform(X_clean_test)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model_transformed = LinearRegression()\n",
    "        model_transformed.fit(X_clean_train_scaled, y_clean_train)\n",
    "        \n",
    "        # Predicciones y residuos\n",
    "        y_pred_transformed = model_transformed.predict(X_clean_test_scaled)\n",
    "        residuals_transformed = y_clean_test - y_pred_transformed\n",
    "        \n",
    "        # Métricas\n",
    "        mse_transformed = mean_squared_error(y_clean_test, y_pred_transformed)\n",
    "        r2_transformed = r2_score(y_clean_test, y_pred_transformed)\n",
    "        rmse_transformed = np.sqrt(mse_transformed)\n",
    "        \n",
    "        # Pruebas de normalidad de residuos\n",
    "        _, shapiro_p = shapiro(residuals_transformed)\n",
    "        _, jb_p = jarque_bera(residuals_transformed)\n",
    "        \n",
    "        # Prueba de heterocedasticidad (usando statsmodels)\n",
    "        X_clean_train_const = sm.add_constant(X_clean_train_scaled)\n",
    "        model_sm_transformed = sm.OLS(y_clean_train, X_clean_train_const).fit()\n",
    "        _, bp_p, _, _ = het_breuschpagan(model_sm_transformed.resid, X_clean_train_const)\n",
    "        \n",
    "        # Almacenar resultados\n",
    "        transformation_results.append({\n",
    "            'Transformación': name,\n",
    "            'MSE': mse_transformed,\n",
    "            'RMSE': rmse_transformed,\n",
    "            'R²': r2_transformed,\n",
    "            'Shapiro_p': shapiro_p,\n",
    "            'JB_p': jb_p,\n",
    "            'BP_p': bp_p,\n",
    "            'Normalidad_Shapiro': 'Sí' if shapiro_p > 0.05 else 'No',\n",
    "            'Homocedasticidad_BP': 'Sí' if bp_p > 0.05 else 'No'\n",
    "        })\n",
    "        \n",
    "        print(f\"MSE: {mse_transformed:.3f}, R²: {r2_transformed:.3f}\")\n",
    "        print(f\"Normalidad (Shapiro): {'Sí' if shapiro_p > 0.05 else 'No'} (p={shapiro_p:.4f})\")\n",
    "        print(f\"Homocedasticidad (BP): {'Sí' if bp_p > 0.05 else 'No'} (p={bp_p:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al aplicar {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(transformation_results)\n",
    "print(\"\\n=== RESUMEN DE TRANSFORMACIONES ===\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a957fb2",
   "metadata": {},
   "source": [
    "### 5.3 Comparación Visual de las Mejores Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3581abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las 3 mejores transformaciones basadas en R²\n",
    "best_transformations = results_df.nlargest(3, 'R²')\n",
    "\n",
    "print(\"Las 3 mejores transformaciones por R²:\")\n",
    "print(best_transformations[['Transformación', 'R²', 'MSE', 'Normalidad_Shapiro', 'Homocedasticidad_BP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones comparativas de residuos\n",
    "# Para cada una de las mejores transformaciones, recrear el modelo y generar gráficos\n",
    "fig, axes = plt.subplots(len(best_transformations) + 1, 4, figsize=(20, 5 * (len(best_transformations) + 1)))\n",
    "\n",
    "models_to_compare = ['Modelo Base'] + best_transformations['Transformación'].tolist()\n",
    "\n",
    "for idx, model_name in enumerate(models_to_compare):\n",
    "    if model_name == 'Modelo Base':\n",
    "        # Usar residuos del modelo base\n",
    "        residuals = residuals_test\n",
    "        y_pred = y_pred_test\n",
    "        y_true = y_test\n",
    "    else:\n",
    "        # Recrear modelo con la transformación específica\n",
    "        scaler = scalers[model_name]\n",
    "        X_clean_train, X_clean_test, y_clean_train, y_clean_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "        \n",
    "        X_clean_train_scaled = scaler.fit_transform(X_clean_train)\n",
    "        X_clean_test_scaled = scaler.transform(X_clean_test)\n",
    "        \n",
    "        model_temp = LinearRegression()\n",
    "        model_temp.fit(X_clean_train_scaled, y_clean_train)\n",
    "        \n",
    "        y_pred = model_temp.predict(X_clean_test_scaled)\n",
    "        y_true = y_clean_test\n",
    "        residuals = y_true - y_pred\n",
    "    \n",
    "    # Gráfico 1: Residuos vs Predichos\n",
    "    axes[idx, 0].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[idx, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[idx, 0].set_title(f'{model_name}\\nResiduos vs Predichos')\n",
    "    axes[idx, 0].set_xlabel('Valores Predichos')\n",
    "    axes[idx, 0].set_ylabel('Residuos')\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 2: Q-Q Plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[idx, 1])\n",
    "    axes[idx, 1].set_title(f'{model_name}\\nQ-Q Plot')\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 3: Histograma de residuos\n",
    "    axes[idx, 2].hist(residuals, bins=20, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    mu, sigma = stats.norm.fit(residuals)\n",
    "    x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "    axes[idx, 2].plot(x, stats.norm.pdf(x, mu, sigma), 'red', linewidth=2)\n",
    "    axes[idx, 2].set_title(f'{model_name}\\nDistribución de Residuos')\n",
    "    axes[idx, 2].set_xlabel('Residuos')\n",
    "    axes[idx, 2].set_ylabel('Densidad')\n",
    "    axes[idx, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 4: Valores reales vs predichos\n",
    "    axes[idx, 3].scatter(y_true, y_pred, alpha=0.6)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[idx, 3].plot([min_val, max_val], [min_val, max_val], 'red', linestyle='--')\n",
    "    axes[idx, 3].set_title(f'{model_name}\\nReal vs Predicho')\n",
    "    axes[idx, 3].set_xlabel('Valores Reales')\n",
    "    axes[idx, 3].set_ylabel('Valores Predichos')\n",
    "    axes[idx, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparación de Calidad de Residuos', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6c4b4",
   "metadata": {},
   "source": [
    "## 6. Resumen y Comparación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen comparativa\n",
    "print(\"=== RESUMEN COMPARATIVO FINAL ===\")\n",
    "print(\"\\nMétricas de Rendimiento:\")\n",
    "summary_metrics = results_df[['Transformación', 'MSE', 'RMSE', 'R²']].copy()\n",
    "summary_metrics = summary_metrics.sort_values('R²', ascending=False)\n",
    "print(summary_metrics.round(4))\n",
    "\n",
    "print(\"\\nCumplimiento de Supuestos:\")\n",
    "assumptions_summary = results_df[['Transformación', 'Normalidad_Shapiro', 'Homocedasticidad_BP']].copy()\n",
    "print(assumptions_summary)\n",
    "\n",
    "# Contar cuántas transformaciones cumplen cada supuesto\n",
    "normality_count = (results_df['Normalidad_Shapiro'] == 'Sí').sum()\n",
    "homoscedasticity_count = (results_df['Homocedasticidad_BP'] == 'Sí').sum()\n",
    "both_count = ((results_df['Normalidad_Shapiro'] == 'Sí') & \n",
    "              (results_df['Homocedasticidad_BP'] == 'Sí')).sum()\n",
    "\n",
    "print(f\"\\nResumen de cumplimiento de supuestos:\")\n",
    "print(f\"- Normalidad de residuos: {normality_count}/{len(results_df)} transformaciones\")\n",
    "print(f\"- Homocedasticidade: {homoscedasticity_count}/{len(results_df)} transformaciones\")\n",
    "print(f\"- Ambos supuestos: {both_count}/{len(results_df)} transformaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a277878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de comparación final\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Gráfico 1: R² por transformación\n",
    "sorted_results = results_df.sort_values('R²')\n",
    "colors = ['red' if x < r2_base else 'green' for x in sorted_results['R²']]\n",
    "bars1 = ax1.barh(range(len(sorted_results)), sorted_results['R²'], color=colors, alpha=0.7)\n",
    "ax1.axvline(x=r2_base, color='blue', linestyle='--', label=f'Modelo Base: {r2_base:.3f}')\n",
    "ax1.set_yticks(range(len(sorted_results)))\n",
    "ax1.set_yticklabels(sorted_results['Transformación'], fontsize=10)\n",
    "ax1.set_xlabel('R² Score')\n",
    "ax1.set_title('R² por Transformación')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 2: MSE por transformación\n",
    "sorted_mse = results_df.sort_values('MSE')\n",
    "colors = ['green' if x < mse_base else 'red' for x in sorted_mse['MSE']]\n",
    "bars2 = ax2.barh(range(len(sorted_mse)), sorted_mse['MSE'], color=colors, alpha=0.7)\n",
    "ax2.axvline(x=mse_base, color='blue', linestyle='--', label=f'Modelo Base: {mse_base:.3f}')\n",
    "ax2.set_yticks(range(len(sorted_mse)))\n",
    "ax2.set_yticklabels(sorted_mse['Transformación'], fontsize=10)\n",
    "ax2.set_xlabel('MSE')\n",
    "ax2.set_title('MSE por Transformación')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico 3: Scatter R² vs p-valor Shapiro\n",
    "ax3.scatter(results_df['R²'], results_df['Shapiro_p'], alpha=0.7, s=100)\n",
    "ax3.axhline(y=0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "ax3.set_xlabel('R² Score')\n",
    "ax3.set_ylabel('p-valor Shapiro (Normalidad)')\n",
    "ax3.set_title('R² vs Normalidad de Residuos')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Anotar puntos\n",
    "for i, row in results_df.iterrows():\n",
    "    ax3.annotate(row['Transformación'].split('_')[0], \n",
    "                (row['R²'], row['Shapiro_p']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Gráfico 4: Scatter R² vs p-valor Breusch-Pagan\n",
    "ax4.scatter(results_df['R²'], results_df['BP_p'], alpha=0.7, s=100, color='orange')\n",
    "ax4.axhline(y=0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "ax4.set_xlabel('R² Score')\n",
    "ax4.set_ylabel('p-valor Breusch-Pagan (Homocedasticidad)')\n",
    "ax4.set_title('R² vs Homocedasticidad')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Anotar puntos\n",
    "for i, row in results_df.iterrows():\n",
    "    ax4.annotate(row['Transformación'].split('_')[0], \n",
    "                (row['R²'], row['BP_p']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.suptitle('Análisis Comparativo de Transformaciones', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714a19d",
   "metadata": {},
   "source": [
    "## 7. Interpretación y Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392683cd",
   "metadata": {},
   "source": [
    "### TODO: Completa el análisis respondiendo a estas preguntas:\n",
    "\n",
    "#### 7.1 Análisis de Residuos del Modelo Base:\n",
    "- ¿Los residuos del modelo base siguen una distribución normal?\n",
    "- ¿Se observa heterocedasticidad en los residuos?\n",
    "- ¿Qué patrones identificas en los gráficos de residuos?\n",
    "\n",
    "#### 7.2 Detección de Outliers:\n",
    "- ¿Qué método de detección de outliers fue más efectivo?\n",
    "- ¿Cuántos outliers se detectaron en total?\n",
    "- ¿Cómo interpretas la presencia de estos outliers en el contexto inmobiliario?\n",
    "\n",
    "#### 7.3 Efectividad de las Transformaciones:\n",
    "- ¿Qué transformaciones mejoraron más el cumplimiento de supuestos?\n",
    "- ¿Hay trade-offs entre rendimiento predictivo y cumplimiento de supuestos?\n",
    "- ¿Cuáles transformaciones fueron menos efectivas y por qué?\n",
    "\n",
    "#### 7.4 Comparación de Métodos de Escalado:\n",
    "- ¿Cómo se comparan StandardScaler, MinMaxScaler y las transformaciones más sofisticadas?\n",
    "- ¿Cuándo recomendarías usar cada tipo de transformación?\n",
    "- ¿Qué transformación ofrece el mejor balance entre interpretabilidad y rendimiento?\n",
    "\n",
    "#### 7.5 Recomendaciones Finales:\n",
    "- ¿Qué modelo y transformación recomendarías para producción?\n",
    "- ¿Qué consideraciones adicionales tendrías en cuenta?\n",
    "- ¿Cómo comunicarías estos resultados a stakeholders no técnicos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf436bf3",
   "metadata": {},
   "source": [
    "### Respuestas:\n",
    "\n",
    "**7.1 Análisis de Residuos del Modelo Base:**\n",
    "TODO: Completa con tus observaciones sobre normalidad, heterocedasticidad y patrones\n",
    "\n",
    "**7.2 Detección de Outliers:**\n",
    "TODO: Completa con tus observaciones sobre efectividad de métodos y interpretación\n",
    "\n",
    "**7.3 Efectividad de las Transformaciones:**\n",
    "TODO: Completa con tus observaciones sobre mejores transformaciones y trade-offs\n",
    "\n",
    "**7.4 Comparación de Métodos de Escalado:**\n",
    "TODO: Completa con tus comparaciones y recomendaciones de uso\n",
    "\n",
    "**7.5 Recomendaciones Finales:**\n",
    "TODO: Completa con tus recomendaciones finales y consideraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c936d",
   "metadata": {},
   "source": [
    "## 8. Ejercicio Adicional (Opcional)\n",
    "\n",
    "### TODO: Implementa técnicas adicionales:\n",
    "\n",
    "1. **Transformación logarítmica manual** de variables asimétricas\n",
    "2. **Winsorización** para tratar outliers extremos\n",
    "3. **Regresión robusta** (Huber, RANSAC) para comparar con transformaciones\n",
    "4. **Validación cruzada** para evaluar estabilidad de las transformaciones\n",
    "5. **Análisis de componentes principales** después de transformaciones"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
